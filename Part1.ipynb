{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.spatial as sp\n",
    "import scipy.stats as ss\n",
    "import scipy.sparse as ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word1      word2  similar_human\n",
      "0         cord      smile           0.02\n",
      "1      rooster     voyage           0.04\n",
      "2         noon     string           0.04\n",
      "3        fruit    furnace           0.05\n",
      "4    autograph      shore           0.06\n",
      "..         ...        ...            ...\n",
      "60     cushion     pillow           3.84\n",
      "61    cemetery  graveyard           3.88\n",
      "62  automobile        car           3.92\n",
      "63      midday       moon           3.94\n",
      "64         gem      jewel           3.94\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# data pair from Table 1\n",
    "value1 = ['cord', 'rooster', 'noon', 'fruit', 'autograph', 'automobile', 'mound', 'grin',\\\n",
    "          'asylum', 'asylum', 'graveyard', 'glass', 'boy', 'cushion', 'monk', 'asylum',\\\n",
    "          'coast','grin', 'shore', 'monk', 'boy', 'automobile', 'mound', 'lad', 'forst',\\\n",
    "          'food', 'cemetery', 'shore', 'bird', 'coast', 'furnace', 'crane', 'hill', 'car',\\\n",
    "          'cemetery', 'glass', 'magician', 'crane', 'brother', 'sage', 'oracle', 'bird', 'bird',\\\n",
    "          'food', 'brother', 'asylum', 'furnace', 'magician', 'hill', 'cord', 'glass', 'grin',\\\n",
    "          'serf', 'journey', 'autograph', 'coast', 'forest', 'implement', 'cock', 'boy', 'cushion',\\\n",
    "          'cemetery', 'automobile', 'midday', 'gem']\n",
    "\n",
    "value2 = ['smile', 'voyage', 'string', 'furnace', 'shore', 'wizard', 'stove', 'implement', 'fruit',\\\n",
    "          'monk', 'madhouse', 'magician', 'rooster', 'jewel', 'slave', 'cemetery', 'forest', 'lad',\\\n",
    "          'woodland', 'oracle', 'sage', 'cushion', 'shore', 'wizard', 'graveyard', 'rooster',\\\n",
    "          'woodland', 'voyage', 'woodland', 'hill', 'implement', 'rooster', 'woodland', 'journey',\\\n",
    "          'mound', 'jewel', 'oracle', 'implement', 'lad', 'wizard', 'sage', 'crane', 'cock', 'fruit',\\\n",
    "          'monk', 'madhouse', 'stove', 'wizard', 'mound', 'string', 'tumbler', 'smile', 'slave',\\\n",
    "          'voyage', 'signature', 'shore', 'woodland', 'tool', 'rooster', 'lad', 'pillow', 'graveyard',\\\n",
    "          'car', 'moon', 'jewel']\n",
    "similar_human = [0.02, 0.04, 0.04, 0.05, 0.06, 0.11, 0.14, 0.18, 0.19, 0.39, 0.42, 0.44, 0.44, 0.45,\\\n",
    "                 0.57, 0.79, 0.85, 0.88, 0.90, 0.91, 0.96, 0.97, 0.97, 0.99, 1.00, 1.09, 1.18, 1.22,\\\n",
    "                 1.24, 1.26, 1.37, 1.41, 1.48, 1.55, 1.69, 1.78, 1.82, 2.37, 2.41, 2.46, 2.61, 2.63,\\\n",
    "                 2.63, 2.69, 2.74, 3.04, 3.11, 3.21, 3.29, 3.41, 3.45, 3.46, 3.46, 3.58, 3.59, 3.60,\\\n",
    "                 3.65, 3.66, 3.68, 3.82, 3.84, 3.88, 3.92, 3.94, 3.94]\n",
    "\n",
    "# construct a dataframe reprenseting the data pairs\n",
    "table1 = {'word1': value1, 'word2':value2, 'similar_human':similar_human}\n",
    "df_table1 = pd.DataFrame(data=table1)\n",
    "print(df_table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  count\n",
      "0            one   3292\n",
      "1          would   2714\n",
      "2           said   1961\n",
      "3            new   1635\n",
      "4          could   1601\n",
      "...          ...    ...\n",
      "4995      emerge     18\n",
      "4996     proceed     18\n",
      "4997  remarkably     18\n",
      "4998   compelled     18\n",
      "4999      faster     18\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# get brown corpus\n",
    "b_words = brown.words()\n",
    "\n",
    "# create tokenizer to remove punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# remove stopwords and punctuations\n",
    "filtered_words = [word.lower() for word in b_words if (word.lower() not in stopwords.words('english')) \\\n",
    "                  & (len(tokenizer.tokenize(word)))]\n",
    "\n",
    "# count frequency and get the most common 5000\n",
    "fdist = FreqDist(filtered_words)\n",
    "top_fivek = fdist.most_common(5000)\n",
    "\n",
    "# covert top_fivek to dataframe\n",
    "df_topfivek = pd.DataFrame(top_fivek, columns=['word', 'count'])\n",
    "print(df_topfivek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word1    word2  similar_human\n",
      "0        noon   string           0.04\n",
      "1       coast   forest           0.85\n",
      "2       coast     hill           1.26\n",
      "3         car  journey           1.55\n",
      "4        food    fruit           2.69\n",
      "5       coast    shore           3.60\n",
      "6  automobile      car           3.92\n"
     ]
    }
   ],
   "source": [
    "# construct an empty df for final result\n",
    "df_shared = pd.DataFrame(columns=df_table1.columns)\n",
    "\n",
    "# find the shared pairs\n",
    "for index, row in df_table1.iterrows():\n",
    "    if (row['word1'] in df_topfivek.word.values) & (row['word2'] in df_topfivek.word.values):\n",
    "        df_shared = df_shared.append(row, ignore_index=True)\n",
    "print(df_shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word2vec\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word1    word2  similar_human  \\\n",
      "0        noon   string           0.04   \n",
      "1       coast   forest           0.85   \n",
      "2       coast     hill           1.26   \n",
      "3         car  journey           1.55   \n",
      "4        food    fruit           2.69   \n",
      "5       coast    shore           3.60   \n",
      "6  automobile      car           3.92   \n",
      "\n",
      "                                     word1_embedding  \\\n",
      "0  [-0.18164062, -0.026123047, -0.20996094, -0.01...   \n",
      "1  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "2  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "3  [0.13085938, 0.008422852, 0.033447266, -0.0588...   \n",
      "4  [-0.18164062, 0.16503906, -0.16601562, 0.35742...   \n",
      "5  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "6  [0.13183594, 0.060546875, 0.0154418945, 0.1347...   \n",
      "\n",
      "                                     word2_embedding  \n",
      "0  [-0.042236328, 0.080078125, -0.18652344, -0.07...  \n",
      "1  [0.33789062, 0.17089844, -0.0028381348, 0.0354...  \n",
      "2  [0.09472656, 0.24414062, 0.0390625, 0.12158203...  \n",
      "3  [0.05029297, 0.265625, -0.15136719, -0.0385742...  \n",
      "4  [-0.05834961, 0.067871094, -0.053955078, 0.333...  \n",
      "5  [-0.024658203, 0.13378906, -0.31640625, -0.118...  \n",
      "6  [0.13085938, 0.008422852, 0.033447266, -0.0588...  \n"
     ]
    }
   ],
   "source": [
    "# add embedding to df_shared\n",
    "df_shared['word1_embedding'] = df_shared['word1'].apply(lambda x: model[x])\n",
    "df_shared['word2_embedding'] = df_shared['word2'].apply(lambda x: model[x])\n",
    "print(df_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word1    word2  similar_human  \\\n",
      "0        noon   string           0.04   \n",
      "1       coast   forest           0.85   \n",
      "2       coast     hill           1.26   \n",
      "3         car  journey           1.55   \n",
      "4        food    fruit           2.69   \n",
      "5       coast    shore           3.60   \n",
      "6  automobile      car           3.92   \n",
      "\n",
      "                                     word1_embedding  \\\n",
      "0  [-0.18164062, -0.026123047, -0.20996094, -0.01...   \n",
      "1  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "2  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "3  [0.13085938, 0.008422852, 0.033447266, -0.0588...   \n",
      "4  [-0.18164062, 0.16503906, -0.16601562, 0.35742...   \n",
      "5  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "6  [0.13183594, 0.060546875, 0.0154418945, 0.1347...   \n",
      "\n",
      "                                     word2_embedding  similar_cosine  \n",
      "0  [-0.042236328, 0.080078125, -0.18652344, -0.07...        0.021655  \n",
      "1  [0.33789062, 0.17089844, -0.0028381348, 0.0354...        0.236098  \n",
      "2  [0.09472656, 0.24414062, 0.0390625, 0.12158203...        0.161158  \n",
      "3  [0.05029297, 0.265625, -0.15136719, -0.0385742...        0.098496  \n",
      "4  [-0.05834961, 0.067871094, -0.053955078, 0.333...        0.374093  \n",
      "5  [-0.024658203, 0.13378906, -0.31640625, -0.118...        0.508367  \n",
      "6  [0.13085938, 0.008422852, 0.033447266, -0.0588...        0.583837  \n"
     ]
    }
   ],
   "source": [
    "# calculate cosine distance\n",
    "for index, row in df_shared.iterrows():\n",
    "    df_shared.at[index, 'similar_cosine'] = 1 - sp.distance.cosine(row['word1_embedding'], row['word2_embedding'])\n",
    "print(df_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word1    word2  similar_human  \\\n",
      "0        noon   string         0.0100   \n",
      "1       coast   forest         0.2125   \n",
      "2       coast     hill         0.3150   \n",
      "3         car  journey         0.3875   \n",
      "4        food    fruit         0.6725   \n",
      "5       coast    shore         0.9000   \n",
      "6  automobile      car         0.9800   \n",
      "\n",
      "                                     word1_embedding  \\\n",
      "0  [-0.18164062, -0.026123047, -0.20996094, -0.01...   \n",
      "1  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "2  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "3  [0.13085938, 0.008422852, 0.033447266, -0.0588...   \n",
      "4  [-0.18164062, 0.16503906, -0.16601562, 0.35742...   \n",
      "5  [0.02709961, 0.107910156, -0.31054688, -0.1533...   \n",
      "6  [0.13183594, 0.060546875, 0.0154418945, 0.1347...   \n",
      "\n",
      "                                     word2_embedding  similar_cosine  \n",
      "0  [-0.042236328, 0.080078125, -0.18652344, -0.07...        0.021655  \n",
      "1  [0.33789062, 0.17089844, -0.0028381348, 0.0354...        0.236098  \n",
      "2  [0.09472656, 0.24414062, 0.0390625, 0.12158203...        0.161158  \n",
      "3  [0.05029297, 0.265625, -0.15136719, -0.0385742...        0.098496  \n",
      "4  [-0.05834961, 0.067871094, -0.053955078, 0.333...        0.374093  \n",
      "5  [-0.024658203, 0.13378906, -0.31640625, -0.118...        0.508367  \n",
      "6  [0.13085938, 0.008422852, 0.033447266, -0.0588...        0.583837  \n"
     ]
    }
   ],
   "source": [
    "# faltten the similar_human as it's from 0-4 and similar_cosine is from 0 to 1\n",
    "df_shared['similar_human'] = df_shared['similar_human']/4\n",
    "print(df_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_correlation : 0.9440722068532515\n",
      "p-value : 0.0013784318897755413\n"
     ]
    }
   ],
   "source": [
    "# calculate the pearson correlation\n",
    "pearson_correlation = ss.pearsonr(df_shared['similar_human'], df_shared['similar_cosine'])[0]\n",
    "p_value = ss.pearsonr(df_shared['similar_human'], df_shared['similar_cosine'])[1]\n",
    "print('pearson_correlation : ' + str(pearson_correlation))\n",
    "print('p-value : ' + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asylum' 'autograph' 'cemetery' 'cock' 'cord' 'crane' 'cushion' 'forst'\n",
      " 'furnace' 'gem' 'graveyard' 'grin' 'implement' 'jewel' 'lad' 'madhouse'\n",
      " 'magician' 'midday' 'monk' 'mound' 'oracle' 'pillow' 'rooster' 'sage'\n",
      " 'serf' 'signature' 'stove' 'tumbler' 'voyage' 'wizard' 'woodland']\n"
     ]
    }
   ],
   "source": [
    "# find additional values not in top 5 k\n",
    "import numpy as np\n",
    "\n",
    "additionalW = []\n",
    "for index, row in df_table1.iterrows():\n",
    "    if (row['word1'] not in df_topfivek.word.values):\n",
    "        additionalW.append(row['word1'])\n",
    "    if (row['word2'] not in df_topfivek.word.values):\n",
    "        additionalW.append(row['word2'])\n",
    "additionalW = np.unique(additionalW)\n",
    "print(additionalW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 15, 5, 6, 5, 8, None, 11, 4, 7, 13, 4, 1, 6, 1, 4, 5, 16, 11, 2, 8, 3, 2, None, 6, 15, 2, 17, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# frequency for new words\n",
    "additionalC = []\n",
    "for aw in additionalW:\n",
    "    freq = fdist.get(aw)\n",
    "    additionalC.append(fdist.get(aw))\n",
    "print(additionalC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "bigrams = ngrams(brown.words(), 2)\n",
    "bigrams_freq = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word   count\n",
      "0          one  3292.0\n",
      "1        would  2714.0\n",
      "2         said  1961.0\n",
      "3          new  1635.0\n",
      "4        could  1601.0\n",
      "...        ...     ...\n",
      "5024     stove    15.0\n",
      "5025   tumbler     2.0\n",
      "5026    voyage    17.0\n",
      "5027    wizard     3.0\n",
      "5028  woodland     2.0\n",
      "\n",
      "[5029 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# new W\n",
    "df_additionalW = pd.DataFrame({'word': additionalW, 'count': additionalC}, columns=['word', 'count'])\n",
    "# remove additional word not in corpus at all\n",
    "df_additionalW.dropna(subset=['count'], inplace=True)\n",
    "\n",
    "df_W = df_topfivek\n",
    "df_W = df_W.append(df_additionalW, ignore_index=True)\n",
    "print(df_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct M1\n",
    "# df_M1 = pd.DataFrame(columns=['l_word', 'f_word', 'l_word_freq', 'f_word_freq', 'bigram_freq'])\n",
    "\n",
    "# for l in df_W.word:\n",
    "#     for f in df_W.word:\n",
    "#         if l != f:\n",
    "#             row = {'l_word':l , 'f_word':f, 'l_word_freq': df_W[df_W['word'] == l]['count'].values[0],\\\n",
    "#                    'f_word_freq': df_W[df_W['word'] == f]['count'].values[0], 'bigram_freq': bigrams_freq[(l, f)]}\n",
    "#             df_M1 = df_M1.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros(shape=(df_W.shape[0],df_W.shape[0]))\n",
    "\n",
    "for i in range(df_W.shape[0]):\n",
    "    for j in range(df_W.shape[0]):\n",
    "        l_word = df_W.iloc[i]['word']\n",
    "        f_word = df_W.iloc[j]['word']\n",
    "        bigram_freq = bigrams_freq[(l_word, f_word)]\n",
    "        matrix[i][j] = bigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_W.iloc[0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         l_word     f_word  l_word_freq  f_word_freq bigram_freq        PMI\n",
      "0           one      would       3292.0       2714.0          17 -13.172216\n",
      "1         would       said       2714.0       1961.0           0   0.000000\n",
      "2          said        new       1961.0       1635.0           0   0.000000\n",
      "3           new      could       1635.0       1601.0           0   0.000000\n",
      "4         could       time       1601.0       1598.0           1 -14.754892\n",
      "...         ...        ...          ...          ...         ...        ...\n",
      "5079     pillow   cemetery          8.0         15.0           0   0.000000\n",
      "5080   cemetery  graveyard         15.0          7.0           0   0.000000\n",
      "5081  graveyard     midday          7.0          5.0           0   0.000000\n",
      "5082     midday        gem          5.0          4.0           0   0.000000\n",
      "5083        gem      jewel          4.0          1.0           0   0.000000\n",
      "\n",
      "[5084 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# calculate M1+\n",
    "import math\n",
    "\n",
    "M1_PMI = []\n",
    "\n",
    "for index, row in df_M1.iterrows():\n",
    "    pmi = row['bigram_freq'] / (row['l_word_freq'] * row['f_word_freq'])\n",
    "    if not (math.isnan(pmi) or pmi == 0):\n",
    "        pmi = math.log(pmi)\n",
    "    M1_PMI.append(pmi)\n",
    "\n",
    "df_M1['PMI'] = M1_PMI\n",
    "print(df_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_word</th>\n",
       "      <th>f_word</th>\n",
       "      <th>l_word_freq</th>\n",
       "      <th>f_word_freq</th>\n",
       "      <th>bigram_freq</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would</td>\n",
       "      <td>said</td>\n",
       "      <td>2714.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  l_word f_word  l_word_freq  f_word_freq bigram_freq  PMI\n",
       "1  would   said       2714.0       1961.0           0  0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use PCA for r2\n",
    "from pca import pca\n",
    "\n",
    "# for word in df_M1.l_word.unique():\n",
    "#     df_M1[df_M1['l_word'] == word] \n",
    "    \n",
    "# df_M1p = df_M1[['l_word', 'f_word', 'PMI']]\n",
    "df_M1[df_M1['l_word'] == 'would'] \n",
    "\n",
    "# M2100 = pca(n_components=10)\n",
    "# results_M2100 = M2100.fit_transform(df_M1p)\n",
    "# M210 = pca(n_components=100)\n",
    "# M2300 = pca(n_components=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      value1 value2     value3   value4\n",
      "0    Algeria  dinar     Angola   kwanza\n",
      "1    Algeria  dinar  Argentina     peso\n",
      "2    Algeria  dinar    Armenia     dram\n",
      "3    Algeria  dinar     Brazil     real\n",
      "4    Algeria  dinar   Bulgaria      lev\n",
      "..       ...    ...        ...      ...\n",
      "861  Vietnam   dong     Russia    ruble\n",
      "862  Vietnam   dong     Sweden    krona\n",
      "863  Vietnam   dong   Thailand     baht\n",
      "864  Vietnam   dong    Ukraine  hryvnia\n",
      "865  Vietnam   dong        USA   dollar\n",
      "\n",
      "[866 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# read txt file\n",
    "df_capital = pd.read_csv('semantic2.txt', sep=\" \", header=None)\n",
    "df_capital.columns = ['value1', 'value2', 'value3', 'value4']\n",
    "print(df_capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                value1  \\\n",
      "0    [-0.171875, 0.10205078, 0.24316406, 0.10449219...   \n",
      "1    [-0.171875, 0.10205078, 0.24316406, 0.10449219...   \n",
      "2    [-0.171875, 0.10205078, 0.24316406, 0.10449219...   \n",
      "3    [-0.171875, 0.10205078, 0.24316406, 0.10449219...   \n",
      "4    [-0.171875, 0.10205078, 0.24316406, 0.10449219...   \n",
      "..                                                 ...   \n",
      "861  [0.21484375, 0.22167969, 0.007171631, 0.177734...   \n",
      "862  [0.21484375, 0.22167969, 0.007171631, 0.177734...   \n",
      "863  [0.21484375, 0.22167969, 0.007171631, 0.177734...   \n",
      "864  [0.21484375, 0.22167969, 0.007171631, 0.177734...   \n",
      "865  [0.21484375, 0.22167969, 0.007171631, 0.177734...   \n",
      "\n",
      "                                                value2  \\\n",
      "0    [0.0703125, 0.19726562, -0.19628906, 0.1708984...   \n",
      "1    [0.0703125, 0.19726562, -0.19628906, 0.1708984...   \n",
      "2    [0.0703125, 0.19726562, -0.19628906, 0.1708984...   \n",
      "3    [0.0703125, 0.19726562, -0.19628906, 0.1708984...   \n",
      "4    [0.0703125, 0.19726562, -0.19628906, 0.1708984...   \n",
      "..                                                 ...   \n",
      "861  [0.059570312, 0.123046875, -0.18164062, 0.0373...   \n",
      "862  [0.059570312, 0.123046875, -0.18164062, 0.0373...   \n",
      "863  [0.059570312, 0.123046875, -0.18164062, 0.0373...   \n",
      "864  [0.059570312, 0.123046875, -0.18164062, 0.0373...   \n",
      "865  [0.059570312, 0.123046875, -0.18164062, 0.0373...   \n",
      "\n",
      "                                                value3  \\\n",
      "0    [-0.09765625, 0.00065612793, 0.25390625, 0.020...   \n",
      "1    [0.030517578, -0.010253906, 0.234375, 0.108398...   \n",
      "2    [0.027832031, -0.08203125, 0.24804688, 0.22558...   \n",
      "3    [-0.034423828, 0.05102539, 0.1328125, 0.010009...   \n",
      "4    [0.23046875, -0.012268066, 0.04272461, 0.09814...   \n",
      "..                                                 ...   \n",
      "861  [-0.07714844, -0.008605957, 0.122558594, 0.192...   \n",
      "862  [0.099121094, -0.06933594, 0.24511719, 0.04443...   \n",
      "863  [0.10986328, -0.1171875, 0.010009766, 0.238281...   \n",
      "864  [-0.026367188, -0.06201172, 0.06933594, 0.2695...   \n",
      "865  [-0.24316406, -0.008544922, 0.20507812, 0.0527...   \n",
      "\n",
      "                                                value4  \n",
      "0    [-0.140625, 0.056640625, -0.08251953, 0.216796...  \n",
      "1    [-0.30664062, -0.36328125, -0.114746094, 0.285...  \n",
      "2    [-0.20605469, -0.21386719, -0.29296875, 0.1611...  \n",
      "3    [0.115234375, -0.080566406, -0.078125, 0.12060...  \n",
      "4    [-0.14453125, -0.08544922, -0.13183594, 0.3535...  \n",
      "..                                                 ...  \n",
      "861  [-0.3203125, -0.57421875, -0.625, 0.5390625, 0...  \n",
      "862  [-0.049804688, -0.4765625, -0.34570312, 0.3300...  \n",
      "863  [0.21191406, -0.1328125, -0.03466797, 0.419921...  \n",
      "864  [-0.15527344, -0.30078125, -0.296875, 0.371093...  \n",
      "865  [-0.2734375, -0.28710938, -0.22167969, 0.10498...  \n",
      "\n",
      "[866 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# add embedding to df_shared\n",
    "df_capital_vector = df_capital.applymap(lambda x: model[x])\n",
    "print(df_capital_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dinars', 'dinars', 'dinars', 'dinars', 'riyal', 'dinars', 'dinars', 'riyal', 'dinars', 'dinars', 'dinars', 'dinars', 'dinars', 'dinars', 'dinars', 'dinars', 'Dinar', 'riyal', 'dinars', 'dinars', 'dinars', 'dinars', 'Iraqi_dinar', 'dinars', 'dinars', 'dinars', 'dinars', 'dinars', 'dinars', 'kwanzas', 'kwanzas', 'kwanzas', 'Angolan', 'Angolan', 'Angolan', 'kwanzas', 'Angolan', 'Angolan', 'Angolan', 'kwanzas', 'kwanzas', 'Angolan', 'Angolan', 'Angolan', 'Angolan', 'Angolan', 'Angolan', 'kwanzas', 'kwanzas', 'Angolan', 'Angolan', 'Talatona_Convention_Centre', 'Angolan', 'Angolan', 'Angolan', 'Angolan', 'Angolan', 'kwanzas', 'pesos', 'pesos', 'pesos', 'Argentine', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'Colombian_peso', 'pesos', 'pesos', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'drams', 'Naomi_Campbell_Hospitalized', 'Brazilian', 'Brazilian', 'Alexandre_Lintz_chief', 'Brazilian', 'Brazilian', 'genuine', 'Real', 'Liquidez_Corretora', 'Real', 'coach_Rogerio_Lourenco', 'Brazilian', 'Brazilian', 'Brazilian', 'Real', 'genuine', 'Naomi_Campbell_Hospitalized', 'genuine', 'Brazilian', 'Real', 'Alexandre_Lintz_chief', 'Brazilian', 'Liquidez_Corretora', 'Brazilian', 'Brazilian', 'genuine', 'genuine', 'genuine', 'Brazilian', 'Bulgarian', 'levs', 'levs', 'levs', 'levs', 'levs', 'Bulgarian', 'Bulgarian', 'Bulgarian', 'levs', 'levs', 'levs', 'levs', 'Bulgarian', 'Bulgarian', 'leva', 'levs', 'levs', 'levs', 'levs', 'Bulgarian', 'Bulgarian', 'levs', 'Bulgarian', 'leva', 'Bulgarian', 'levs', 'levs', 'levs', 'riels', 'riels', 'Cambodian', 'Cambodian', 'Cambodian', 'riels', 'Cambodian', 'riels', 'riels', 'Cambodian', 'Cambodian', 'Cambodian', 'riels', 'riels', 'Cambodian', 'riels', 'Cambodian', 'Cambodian', 'Cambodian', 'riels', 'Cambodian', 'Phnom_Penh', 'riels', 'Cambodian', 'riels', 'Cambodian', 'Phnom_Penh', 'Cambodian', 'Cambodian', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'loonie', 'kunas', 'Croatian', 'kunas', 'kunas', 'kunas', 'kunas', 'kunas', 'Croatian', 'kunas', 'kunas', 'Zagreb', 'kunas', 'Zagreb', 'kunas', 'kunas', 'kunas', 'kunas', 'kunas', 'kunas', 'kunas', 'Zagreb', 'kunas', 'kunas', 'kunas', 'kunas', 'kunas', 'kunas', 'Zagreb', 'Zagreb', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'krona', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish_krone', 'Danish', 'Danish', 'Danish', 'Danish', 'krona', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'Danish', 'European', 'European', 'European', 'European', 'European', 'European', 'Swiss_franc', 'European', 'European', 'European', 'European', 'European', 'Swiss_franc', 'euro_zone', 'euro_zone', 'European', 'Swiss_franc', 'euro_zone', 'European', 'European', 'European', 'European', 'European', 'European', 'European', 'European', 'euro_zone', 'European', 'euro_zone', 'forints', 'HUF', 'forints', 'forints', 'forints', 'forints', 'zloty', 'zloty', 'forints', 'forints', 'forints', 'forints', 'forints', 'zloty', 'zloty', 'forints', 'forints', 'HUF', 'HUF', 'forints', 'forints', 'forints', 'forints', 'forints', 'HUF', 'forints', 'forints', 'zloty', 'forints', 'rupees', 'rupees', 'Rupee', 'rupees', 'rupees', 'rupees', 'Rupee', 'Rupee', 'rupees', 'Rupee', 'Rupee', 'Rupee', 'depreciating_rupee', 'Rupee', 'Rupee', 'rupees', 'Rupee', 'Rupee', 'rupees', 'Rupee', 'Rupee', 'rupees', 'Rupee', 'Rupee', 'Rupee', 'rupees', 'Rupee', 'Rupee', 'rupees', 'rials', 'rials', 'Tehran', 'rials', 'rials', 'Tehran', 'rials', 'Tehran', 'rials', 'Iranians', 'rials', 'Tehran', 'Tehran', 'rials', 'rials', 'Tehran', 'Tehran', 'Iranians', 'rials', 'rials', 'Islamic_Republic', 'Iranians', 'Tehran', 'Tehran', 'rials', 'Tehran', 'rials', 'Tehran', 'rials', 'trillion_yen', '¥', '¥', 'Japanese', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', '¥', 'Japanese', '¥', '¥', '¥', 'trillion_yen', '¥', '¥', '¥', '¥', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Won', 'Seung_Hi', 'Won', 'Won', 'Won', 'triumphed', 'EEK', 'EEK', 'Latvian', 'Latvian', 'Latvian', 'lati', 'EEK', 'EEK', 'hryvnia', 'litas', 'EEK', 'Latvian', 'Latvian', 'Latvian', 'Latvian', 'EEK', 'lati', 'Latvian', 'lati', 'Latvian', 'EEK', 'EEK', 'rubles', 'EEK', 'EEK', 'Latvian', 'Latvian', 'LVL', 'litas', 'litai', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'litai', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'litai', 'levs', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'litai', 'Lithuanian', 'litai', 'Lithuanian', 'Lithuanian', 'litai', 'levs', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'Lithuanian', 'litai', 'FYROM', 'denars', 'FYROM', 'Denar', 'Denar', 'denars', 'FYROM', 'denars', 'Denar', 'Skopje', 'Skopje', 'FYROM', 'denars', 'denars', 'Denar', 'denars', 'Denar', 'denars', 'denars', 'denars', 'FYROM', 'denars', 'FYROM', 'denars', 'denars', 'denars', 'denars', 'Denar', 'Denar', 'Malaysian', 'RM4', 'Malaysian', 'Malaysian', 'RM###', 'RM1', 'RM###', 'Malaysian', 'RM##', 'Malaysian', 'RM##', 'RM4', 'RM###', 'Malaysian', 'RM4', 'Malaysian', 'Malaysian', 'RM###', 'RM4', 'RM###', 'Malaysian', 'Malaysian', 'RM###', 'Malaysian', 'RM###', 'RM###', 'RM###', 'Malaysian', 'Malaysian', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'pesos', 'Lagos', 'Edo_State', 'Edo_State', 'Naira', 'Naira', 'Edo_State', 'Abia_State', 'Abia_State', 'Naira', 'Naira', 'Naira', 'Edo_State', 'Edo_State', 'Edo_State', 'Nigerian', 'Naira', 'Naira', 'Naira', 'Naira', 'Lagos', 'Naira', 'Edo_State', 'Naira', 'Naira', 'Abia', 'Abia', 'Naira', 'Naira', 'Naira', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'zlotys', 'Romanian', 'hryvnia', 'Romanian', 'Romanian_leu', 'forint', 'Romanian_leu', 'forint', 'forint', 'forint', 'Polish_zloty', 'hryvnia', 'Polish_zloty', 'Romanian', 'hryvnia', 'hryvnia', 'hryvnia', 'hryvnia', 'hryvnia', 'Romanian', 'Romanian', 'Romanian', 'Romanian', 'zloty', 'forint', 'Polish_zloty', 'hryvnia', 'hryvnia', 'Romanian', 'hryvnia', 'Kremlin', 'Kremlin', 'Russian_ruble', 'Kremlin', 'Kremlin', 'rubles', 'Kremlin', 'Kremlin', 'Russian_ruble', 'Kremlin', 'Kremlin', 'Kremlin', 'Kremlin', 'Kremlin', 'rubles', 'Kremlin', 'Kremlin', 'rubles', 'rubles', 'Kremlin', 'Kremlin', 'Kremlin', 'Kremlin', 'Kremlin', 'Kremlin', 'Kremlin', 'Moscow', 'rubles', 'Kremlin', 'krone', 'krone', 'kronor', 'krone', 'krone', 'kronor', 'krone', 'krone', 'Swedish', 'krone', 'krone', 'krone', 'krone', 'krone', 'krone', 'krone', 'krone', 'Swedish', 'krone', 'krone', 'krone', 'krone', 'krone', 'krone', 'krone', 'Swedish', 'krone', 'krone', 'krone', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt##', 'Bt1', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt##', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt###', 'Bt##', 'ruble', 'ruble', 'ruble', 'hryvna', 'hryvnias', 'ruble', 'Kyiv', 'ruble', 'Ukrainian', 'Kiev', 'ruble', 'ruble', 'Prime_Minister_Yulia_Timoshenko', 'ruble', 'Kiev', 'hryvnias', 'Ukrainian', 'Kiev', 'hryvna', 'ruble', 'ruble', 'Kiev', 'Ukrainian', 'Kyiv', 'ruble', 'ruble', 'hryvna', 'ruble', 'Ukrainian', 'Dollar', 'Dollar', 'loonie', 'Dollar', 'greenback', 'Dollar', 'greenback', 'Dollar', 'Dollar', 'greenback', 'Dollar', 'Dollar', 'Dollar', 'pound_sterling', 'Dollar', 'Dollar', 'Dollar', 'Dollar', 'greenback', 'Dollar', 'pound_sterling', 'Dollar', 'Dollar', 'Dollar', 'Dollar', 'Dollar', 'Dollar', 'greenback', 'VND', 'Nam', 'VND', 'Nam', 'Canh', 'Nam', 'VND', 'Thinh', 'VND2', 'Binh', 'Thinh', 'Nam', 'VND', 'VND5', 'VND', 'VND', 'Nam', 'Nam', 'Vietnamese', 'VND', 'Nam', 'Hanoi', 'VND5', 'Phong', 'Thinh', 'VND2', 'VND', 'Canh', 'VND##']\n"
     ]
    }
   ],
   "source": [
    "# df_capital['predict'] = (df_capital['value1']+df_capital['value2']-df_capital['value3']).equals(df_capital['value4'])\n",
    "# model.most_similar(positive=['Algeria', 'Angola'], negative=['dinar'])[0]\n",
    "\n",
    "predict = []\n",
    "\n",
    "for index, row in df_capital.iterrows():\n",
    "    predict.append(model.most_similar(positive=[row['value1'], row['value2']], negative=[row['value3']])[0][0])\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({'predict': predict}, columns=['predict'])\n",
    "df_result['test'] = df_capital['value4'].equals(df_result['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [predict, test]\n",
       "Index: []"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result[df_result['test'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start actual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semantic = pd.read_csv('semantic.txt', sep=\" \", header=None)\n",
    "df_semantic.columns = ['value1', 'value2', 'value3', 'value4']\n",
    "\n",
    "df_syntactic = pd.read_csv('syntactic.txt', sep=\" \", header=None)\n",
    "df_syntactic.columns = ['value1', 'value2', 'value3', 'value4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embedding to df_shared\n",
    "df_semantic_vector = df_semantic.applymap(lambda x: model[x])\n",
    "\n",
    "df_syntactic_vector = df_syntactic.applymap(lambda x: model[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semantic_vector['prediction'] = df_semantic_vector['value1'] + df_semantic_vector['value2'] - df_semantic_vector['value3']\n",
    "\n",
    "df_syntactic_vector['prediction'] = df_syntactic_vector['value1'] + df_syntactic_vector['value2'] - df_syntactic_vector['value3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semantic_vector['similarity'] = df_semantic_vector.apply(lambda row: 1 - sp.distance.cosine(row['value4'], row['prediction']), axis=1)\n",
    "\n",
    "df_syntactic_vector['similarity'] = df_syntactic_vector.apply(lambda row: 1 - sp.distance.cosine(row['value4'], row['prediction']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic has 0.42843477221092907 positive cosine similarity rate\n",
      "syntactic has 0.17938568714376188 positive cosine similarity rate\n"
     ]
    }
   ],
   "source": [
    "semantic_pos = df_semantic_vector[df_semantic_vector['similarity'] > 0].shape[0]\n",
    "syntactic_pos = df_syntactic_vector[df_syntactic_vector['similarity'] > 0].shape[0]\n",
    "\n",
    "print('semantic has ' + str(semantic_pos/df_semantic_vector.shape[0])+ ' positive cosine similarity rate')\n",
    "print('syntactic has ' + str(syntactic_pos/df_syntactic_vector.shape[0])+ ' positive cosine similarity rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
